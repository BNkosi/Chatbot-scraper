{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open website home page\n",
    "\n",
    "URL = 'https://explore-datascience.net/'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Extract all urls from the website page\n",
    "\n",
    "def get_urls(soup):\n",
    "    \"\"\"\n",
    "    Function returns a list of all url links from a BeautifulSoup object.\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.get('href') != None:\n",
    "            urls.append(link.get('href'))\n",
    "    urls = [x for x in urls if 'https' in x]\n",
    "    urls = list(set(urls))\n",
    "    return urls\n",
    "\n",
    "urls_list = page_urls(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def website_links(urls_list):\n",
    "    urls = [x for x in urls_list if '.pdf' not in x]\n",
    "    pdfs = [x for x in urls_list if '.pdf' in x]\n",
    "    new_urls = []\n",
    "    for i in range(len(urls)):\n",
    "        page = requests.get(urls[i])\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        pg_urls = get_urls(soup)\n",
    "        if len(pg_urls) > 0:\n",
    "            pdfs.extend([x for x in pg_urls if '.pdf' in x and x not in pdfs])\n",
    "            new_urls.extend([x for x in pg_urls if x not in urls \n",
    "                             and x not in new_urls \n",
    "                             and x not in pdfs \n",
    "                             and 'explore-datascience' in x])\n",
    "    urls.extend(new_urls)\n",
    "    return urls, len(new_urls), pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all urls and pdfs from the website.\n",
    "\n",
    "urls, new_urls, pdfs = website_links(urls_list)\n",
    "if new_urls > 0:\n",
    "    urls, new_urls, pdfs = website_links(urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1 of 53\n",
      "Completed 2 of 53\n",
      "Completed 3 of 53\n",
      "Completed 4 of 53\n",
      "Completed 5 of 53\n",
      "Completed 6 of 53\n",
      "Completed 7 of 53\n",
      "Completed 8 of 53\n",
      "Completed 9 of 53\n",
      "Completed 10 of 53\n",
      "Completed 11 of 53\n",
      "Completed 12 of 53\n",
      "Completed 13 of 53\n",
      "Completed 14 of 53\n",
      "Completed 15 of 53\n",
      "Completed 16 of 53\n",
      "Completed 17 of 53\n",
      "Completed 18 of 53\n",
      "Completed 19 of 53\n",
      "Completed 20 of 53\n",
      "Completed 21 of 53\n",
      "Completed 22 of 53\n",
      "Completed 23 of 53\n",
      "Completed 24 of 53\n",
      "Completed 25 of 53\n",
      "Completed 26 of 53\n",
      "Completed 27 of 53\n",
      "Completed 28 of 53\n",
      "Completed 29 of 53\n",
      "Completed 30 of 53\n",
      "Completed 31 of 53\n",
      "Completed 32 of 53\n",
      "Completed 33 of 53\n",
      "Completed 34 of 53\n",
      "Completed 35 of 53\n",
      "Completed 36 of 53\n",
      "Completed 37 of 53\n",
      "Completed 38 of 53\n",
      "Completed 39 of 53\n",
      "Completed 40 of 53\n",
      "Completed 41 of 53\n",
      "Completed 42 of 53\n",
      "Completed 43 of 53\n",
      "Completed 44 of 53\n",
      "Completed 45 of 53\n",
      "Completed 46 of 53\n",
      "Completed 47 of 53\n",
      "Completed 48 of 53\n",
      "Completed 49 of 53\n",
      "Completed 50 of 53\n",
      "Completed 51 of 53\n",
      "Completed 52 of 53\n",
      "Completed 53 of 53\n"
     ]
    }
   ],
   "source": [
    "# Scrape text data from the website (this excludes the pdfs)\n",
    "\n",
    "for i in range(len(urls)):\n",
    "    page = requests.get(urls[i])\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    f= open(\"page_{}.txt\".format(i),\"w+\")\n",
    "    for items in soup.find_all('section'):\n",
    "        data = '\\n'.join([item.text for item in items.find_all(['h2', 'h3','p', 'li'])])\n",
    "        f.write(data)\n",
    "    f.close()\n",
    "    print('Completed {0} of {1}'.format(i+1, len(urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
